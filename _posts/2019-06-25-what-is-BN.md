---
title: WHAT IS BN?
date: 2019-06-25 15:27:31
---

# BN 简介
## 协变量偏移问题
1. 我们知道，在统计机器学习中算法中，一个常见的问题是协变量偏移(Covariate Shift)，协变量可以看作是输入变量。一般的深度神经网络都要求输入变量在训练数据和测试数据上的分布是相似的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。
2. 传统的深度神经网络在训练时，随着参数的不算更新，中间每一层输入的数据分布往往会和参数更新之前有较大的差异，导致网络要去不断的适应新的数据分布，进而使得训练变得异常困难，我们只能使用一个很小的学习速率和精调的初始化参数来解决这个问题。而且这个中间层的深度越大时，这种现象就越明显。由于是对层间数据的分析，也即是内部（internal），因此这种现象叫做内部协变量偏移(internal Covariate Shift)
## 解决方法
1. 为了解决这个问题，Sergey Ioffe’s 和 Christian Szegedy’s 在2015年首次提出了批量标准化（Batch Normalization，BN）的想法。该想法是：不仅仅对输入层做标准化处理，还要对**每一中间层的输入(激活函数前)**做标准化处理，使得输出服从均值为0，方差为1的正态分布，从而避免内部协变量偏移的问题。之所以称之为批标准化：是因为在训练期间，我们仅通过计算当前层一小批数据的均值和方差来标准化每一层的输入。
## BN 的优点
现在几乎所有的卷积神经网络都会使用批量标准化操作，它可以为我们的网络训练带来一系列的好处。具体如下：
1. 首先，通过对输入和中间网络层的输出进行标准化处理后，减少了内部神经元分布的改变，使降低了不同样本间值域的差异性，得大部分的数据都其处在非饱和区域，从而保证了梯度能够很好的回传，避免了梯度消失和梯度爆炸
2. 其次，通过减少梯度对参数或其初始值尺度的依赖性，使得我们可以使用较大的学习速率对网络进行训练，从而加速网络的收敛
3. 最后，由于在训练的过程中批量标准化所用到的均值和方差是在一小批样本(mini-batch)上计算的，而不是在整个数据集上，所以均值和方差会有一些小噪声产生，同时缩放过程由于用到了含噪声的标准化后的值，所以也会有一点噪声产生，这迫使后面的神经元单元不过分依赖前面的神经元单元。所以，它也可以看作是一种正则化手段，提高了网络的泛化能力，使得我们可以减少或者取消 Dropout，优化网络结构.